{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs - OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install openai python-dotenv\n",
    "!pip install --upgrade openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv # load enviroment variables\n",
    "import os\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OpenAI Key from .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\" \n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Technology Chatbot!\n",
      "Ask any question related to Technology or type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is AI?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: AI stands for Artificial Intelligence. It refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is Cloud Computing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Cloud computing refers to the delivery of computing services, including servers, storage, databases, networking, software, and analytics over the internet (the cloud) to offer faster innovation, flexible resources, and economies of scale. Instead of owning their own computing infrastructure\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye! Thanks for using the chatbot.\n",
      "Welcome to the Technology Chatbot!\n",
      "Ask any question related to Technology or type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is AI?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: AI stands for artificial intelligence, which refers to the\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is Cloud Computing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Cloud computing is the delivery of computing services—including servers\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye! Thanks for using the chatbot.\n"
     ]
    }
   ],
   "source": [
    "def chatbot(messages, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, n, stop=None):\n",
    "    # Generate the response using OpenAI's API\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,  # Correct API method\n",
    "        messages=messages,  # List of messages for the conversation\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n,\n",
    "        stop=stop\n",
    "    )\n",
    "    \n",
    "    # Return the first response correctly\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def tekChatBot(temperature, max_tokens, top_p, frequency_penalty, presence_penalty, n, stop):\n",
    "    print(\"\\n\\nWelcome to the Technology Chatbot!\")\n",
    "    print(\"Ask any question related to Technology or type 'exit' to quit.\\n\")\n",
    "\n",
    "    # Initial messages: set up the assistant's role\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in technology.\"}]\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye! Thanks for using the chatbot.\")\n",
    "            break\n",
    "        \n",
    "        # Add the user's message to the conversation history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Generate a response from the chatbot\n",
    "        response = chatbot(\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            n=n,\n",
    "            stop=stop            \n",
    "        )\n",
    "        \n",
    "        # Display the chatbot's response\n",
    "        print(f\"AI: {response}\\n\")\n",
    "        \n",
    "        # Add the AI's response to the conversation history\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Run the chatbot with the initial parameters\n",
    "tekChatBot(\n",
    "    temperature=0.7,  # Adjust creativity\n",
    "    max_tokens=50,    # Limit response length\n",
    "    top_p=1,          # Top-p sampling\n",
    "    frequency_penalty=0,  # Penalize repetitive responses\n",
    "    presence_penalty=0.6, # Encourage discussing new topics\n",
    "    n=1,               # Number of responses\n",
    "    stop=[\"\\n\"]        # Correct use of stop parameter\n",
    ")\n",
    "\n",
    "tekChatBot(\n",
    "    temperature=0.3,        # Adjust creativity\n",
    "    max_tokens=10,          # Limit response length\n",
    "    top_p=1,                # Top-p sampling\n",
    "    frequency_penalty=1,    # Penalize repetitive words\n",
    "    presence_penalty=0.9,   # Encourage new topics\n",
    "    n=2,                     # Number of responses\n",
    "    stop=['User:']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 1\n",
    "\n",
    "The chatbot's behavior and responses change significantly due to the differences in how OpenAI's models interpret and generate responses. The responses were much shorter, often cut off due to the combined effects of the low token limit and penalties.\n",
    "\n",
    "- First Run Effects:\n",
    "    - temperature=0.7: The responses are more creative and varied because this temperature allows for slightly more randomness.\n",
    "    - max_tokens=50: The responses are concise but provide enough details to explain the core concepts.\n",
    "    - presence_penalty=0.6: This encourages the chatbot to introduce new topics rather than repeating phrases, making the responses richer and less repetitive.\n",
    "    - stop=[\"\\n\"]: The chatbot stops at a new line, preventing overly long responses while keeping them informative.\n",
    "\n",
    "- Second Run Effects:\n",
    "    - temperature=0.3: The responses are more deterministic and focused, as this low temperature reduces randomness.\n",
    "    - max_tokens=10: The responses are extremely brief, often cutting off before the full explanation.\n",
    "    - frequency_penalty=1: The model avoids repeating words or phrases, leading to potentially awkward or incomplete responses due to limited token space.\n",
    "    - presence_penalty=0.9: Strongly encourages the model to introduce new concepts or phrases, further affecting its ability to stay on a repetitive topic.\n",
    "    - stop=[\"User:\"]: The chatbot stops when it encounters \"User:\" in the generated output, limiting long, conversational answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Cloud computing has transformed data storage and access in the digital age by eliminating the need for on-premises data centers. Businesses can now easily scale computing resources without large upfront investments. Cloud providers offer services like IaaS, PaaS, and SaaS', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))\n",
      "\n",
      "AI: Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Cloud', bytes=[67, 108, 111, 117, 100], logprob=-0.025041623, top_logprobs=[]), ChatCompletionTokenLogprob(token=' computing', bytes=[32, 99, 111, 109, 112, 117, 116, 105, 110, 103], logprob=-3.1281633e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' has', bytes=[32, 104, 97, 115], logprob=-0.0044016326, top_logprobs=[]), ChatCompletionTokenLogprob(token=' transformed', bytes=[32, 116, 114, 97, 110, 115, 102, 111, 114, 109, 101, 100], logprob=-0.06718405, top_logprobs=[]), ChatCompletionTokenLogprob(token=' data', bytes=[32, 100, 97, 116, 97], logprob=-0.0047743614, top_logprobs=[]), ChatCompletionTokenLogprob(token=' storage', bytes=[32, 115, 116, 111, 114, 97, 103, 101], logprob=-0.00011486754, top_logprobs=[]), ChatCompletionTokenLogprob(token=' and', bytes=[32, 97, 110, 100], logprob=-0.0040810094, top_logprobs=[]), ChatCompletionTokenLogprob(token=' access', bytes=[32, 97, 99, 99, 101, 115, 115], logprob=-0.041321803, top_logprobs=[]), ChatCompletionTokenLogprob(token=' by', bytes=[32, 98, 121], logprob=-0.837883, top_logprobs=[]), ChatCompletionTokenLogprob(token=' allowing', bytes=[32, 97, 108, 108, 111, 119, 105, 110, 103], logprob=-1.260161, top_logprobs=[]), ChatCompletionTokenLogprob(token=' businesses', bytes=[32, 98, 117, 115, 105, 110, 101, 115, 115, 101, 115], logprob=-0.045399178, top_logprobs=[]), ChatCompletionTokenLogprob(token=' to', bytes=[32, 116, 111], logprob=-7.3458323e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' easily', bytes=[32, 101, 97, 115, 105, 108, 121], logprob=-4.922156, top_logprobs=[]), ChatCompletionTokenLogprob(token=' scale', bytes=[32, 115, 99, 97, 108, 101], logprob=-0.003513595, top_logprobs=[]), ChatCompletionTokenLogprob(token=' computing', bytes=[32, 99, 111, 109, 112, 117, 116, 105, 110, 103], logprob=-0.15090324, top_logprobs=[]), ChatCompletionTokenLogprob(token=' resources', bytes=[32, 114, 101, 115, 111, 117, 114, 99, 101, 115], logprob=-1.9361265e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' without', bytes=[32, 119, 105, 116, 104, 111, 117, 116], logprob=-0.081260525, top_logprobs=[]), ChatCompletionTokenLogprob(token=' high', bytes=[32, 104, 105, 103, 104], logprob=-0.9910017, top_logprobs=[]), ChatCompletionTokenLogprob(token=' upfront', bytes=[32, 117, 112, 102, 114, 111, 110, 116], logprob=-0.13508286, top_logprobs=[]), ChatCompletionTokenLogprob(token=' costs', bytes=[32, 99, 111, 115, 116, 115], logprob=-0.16929694, top_logprobs=[]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.07329115, top_logprobs=[]), ChatCompletionTokenLogprob(token=' Cloud', bytes=[32, 67, 108, 111, 117, 100], logprob=-0.48374647, top_logprobs=[]), ChatCompletionTokenLogprob(token=' providers', bytes=[32, 112, 114, 111, 118, 105, 100, 101, 114, 115], logprob=-0.024675552, top_logprobs=[]), ChatCompletionTokenLogprob(token=' offer', bytes=[32, 111, 102, 102, 101, 114], logprob=-0.00051287946, top_logprobs=[]), ChatCompletionTokenLogprob(token=' I', bytes=[32, 73], logprob=-3.6098387, top_logprobs=[]), ChatCompletionTokenLogprob(token='aaS', bytes=[97, 97, 83], logprob=-4.00813e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=',', bytes=[44], logprob=-1.9361265e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' P', bytes=[32, 80], logprob=-7.465036e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token='aaS', bytes=[97, 97, 83], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token=',', bytes=[44], logprob=-6.511407e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' and', bytes=[32, 97, 110, 100], logprob=-4.4849444e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' S', bytes=[32, 83], logprob=-6.704273e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='aaS', bytes=[97, 97, 83], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token=' services', bytes=[32, 115, 101, 114, 118, 105, 99, 101, 115], logprob=-0.009475687, top_logprobs=[]), ChatCompletionTokenLogprob(token=',', bytes=[44], logprob=-0.1821471, top_logprobs=[]), ChatCompletionTokenLogprob(token=' eliminating', bytes=[32, 101, 108, 105, 109, 105, 110, 97, 116, 105, 110, 103], logprob=-0.17974512, top_logprobs=[]), ChatCompletionTokenLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-1.4378848e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' need', bytes=[32, 110, 101, 101, 100], logprob=-2.677603e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' for', bytes=[32, 102, 111, 114], logprob=-0.00014764121, top_logprobs=[]), ChatCompletionTokenLogprob(token=' maintaining', bytes=[32, 109, 97, 105, 110, 116, 97, 105, 110, 105, 110, 103], logprob=-1.8540466, top_logprobs=[]), ChatCompletionTokenLogprob(token=' on', bytes=[32, 111, 110], logprob=-0.8004197, top_logprobs=[]), ChatCompletionTokenLogprob(token='-pre', bytes=[45, 112, 114, 101], logprob=-0.0003770496, top_logprobs=[]), ChatCompletionTokenLogprob(token='m', bytes=[109], logprob=-0.0001659949, top_logprobs=[]), ChatCompletionTokenLogprob(token='ises', bytes=[105, 115, 101, 115], logprob=-3.1281633e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' data', bytes=[32, 100, 97, 116, 97], logprob=-6.0345924e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' centers', bytes=[32, 99, 101, 110, 116, 101, 114, 115], logprob=-3.7697225e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.10702043, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='Cloud computing has transformed data storage and access by allowing businesses to easily scale computing resources without high upfront costs. Cloud providers offer IaaS, PaaS, and SaaS services, eliminating the need for maintaining on-premises data centers.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat(messages, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, n=1, best_of=1, logprobs=False):\n",
    "    # Generate the response using OpenAI's API\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,  # Correct API method\n",
    "        messages=messages,  # List of messages for the conversation\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n,\n",
    "        stop=None,\n",
    "        # best_of=best_of,\n",
    "        logprobs=logprobs\n",
    "    )\n",
    "\n",
    "    # Return the first response correctly\n",
    "    return response.choices[0]\n",
    "\n",
    "long_text = '''\n",
    "Enter a long text to summarize (Press Enter twice to finish):\n",
    "In the digital age, cloud computing has revolutionized the way we store and access data. \n",
    "Gone are the days when companies had to maintain on-premises data centers with high costs and maintenance requirements. \n",
    "Now, businesses can scale computing resources up or down as needed without significant upfront investments. \n",
    "Cloud providers offer various services, including infrastructure as a service (IaaS), platform as a service (PaaS), and software as a service (SaaS).\n",
    "'''\n",
    "\n",
    "# Initial messages: set up the assistant's role\n",
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes long texts.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following text: {long_text}\"}\n",
    "        ]\n",
    "     \n",
    "# Run the chatbot with the initial parameters\n",
    "response = chat(\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.6,\n",
    "        n=1,\n",
    "        best_of=1,\n",
    "        logprobs=False  # Show the top 3 token probabilities\n",
    "    )\n",
    "\n",
    "# Display the chatbot's response\n",
    "print(f\"AI: {response}\\n\")\n",
    "\n",
    "# Run the chatbot with the others parameters\n",
    "response = chat(\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.6,\n",
    "        n=1,\n",
    "        best_of=2,\n",
    "        logprobs=True  \n",
    "    )\n",
    "\n",
    "# Display the chatbot's response\n",
    "print(f\"AI: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 2\n",
    "Changing the parameters in the OpenAI API significantly affects the output. Increasing temperature adds creativity, while lowering it makes responses more deterministic. max_tokens controls response length, with higher values allowing more complete answers. top_p adjusts diversity by considering tokens within a probability threshold, and lowering it increases variety. frequency_penalty reduces repetitive phrases, and presence_penalty encourages introducing new topics. best_of generates multiple completions and selects the best, improving response quality, while logprobs provides token-level probabilities, offering insights into token selection but not affecting the output directly. Balancing these parameters enables generating tailored, high-quality responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate Text Script: \n",
      "\tOne example where you’d want to ban some words (tokens) from appearing in the results is for moderation purposes.\n",
      "\n",
      "Spanish Translation ...\n",
      "\n",
      "Translated Text - Approach 1:\n",
      "\tUn ejemplo en el que querrías prohibir algunas palabras (tokens) de aparecer en los resultados es por motivos de moderación.\n",
      "\n",
      "Translated Text - Approach 2:\n",
      "\tUn ejemplo donde querrías prohibir algunas palabras (tokens) de aparecer en los resultados es por razones de moderación.\n"
     ]
    }
   ],
   "source": [
    "def translate_text(text, message_history, target_language='Spanish', temperature=0.5, max_tokens=100, top_p=1, frequency_penalty=0, presence_penalty=0,logit_bias=None,  echo=False ):\n",
    "    # Create a prompt for translation\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{text}\"})\n",
    "    \n",
    "    # Call the OpenAI API to generate the translation\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,  \n",
    "        messages=message_history,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        # echo=echo,\n",
    "        logit_bias=logit_bias\n",
    "    )\n",
    "    \n",
    "    # Return the translated text\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "source_text = \"One example where you’d want to ban some words (tokens) from appearing in the results is for moderation purposes.\"\n",
    "target_language = \"Spanish\"\n",
    "\n",
    "print(\"Translate Text Script: \")\n",
    "print(f\"\\t{source_text}\")\n",
    "\n",
    "print(f\"\\n{target_language} Translation ...\")\n",
    "\n",
    "# Tokenize words to get token ID\n",
    "encoding = tiktoken.encoding_for_model(gpt_model)\n",
    "example_token_id = encoding.encode(\"ejemplo\")[0]\n",
    "moderation_token_id = encoding.encode(\"moderación\")[0]\n",
    "\n",
    "# Initial messages: set up the assistant's role\n",
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Translate this into {target_language}\"}\n",
    "        ]\n",
    "\n",
    "# Parameters can be adjusted here for experimentation\n",
    "translation = translate_text(\n",
    "    text=source_text,\n",
    "    message_history=messages,    \n",
    "    target_language=target_language,\n",
    "    temperature=0.5,         # Adjust creativity of translation\n",
    "    max_tokens=100,          # Limit the length of the translation\n",
    "    top_p=1,                 # Control diversity\n",
    "    frequency_penalty=0,     # Avoid repetitive phrases\n",
    "    presence_penalty=0,      # Encourage new expressions\n",
    "    logit_bias={example_token_id: 5, moderation_token_id: 5},  # No bias in token selection\n",
    "    echo=False              # Prevent input repetition    \n",
    ")\n",
    "\n",
    "print(\"\\nTranslated Text - Approach 1:\")\n",
    "print(f\"\\t{translation}\")\n",
    "\n",
    "# Parameters can be adjusted here for experimentation\n",
    "translation = translate_text(\n",
    "    text=source_text,\n",
    "    message_history=messages,\n",
    "    target_language=target_language,\n",
    "    temperature=0.3,         # Minimal creativity, focused output\n",
    "    max_tokens=50,           # Short and concise translation\n",
    "    top_p=1,                 # Standard token selection\n",
    "    frequency_penalty=0,     # No repetition penalty\n",
    "    presence_penalty=0.2,    # Minimal penalty for new words\n",
    "    logit_bias={example_token_id: -100, moderation_token_id: -100},  \n",
    "    echo=True                 \n",
    ")\n",
    "\n",
    "print(\"\\nTranslated Text - Approach 2:\")\n",
    "print(f\"\\t{translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 3\n",
    "- Approach 1: The translation includes “ejemplo” and “moderación” as intended due to the positive logit_bias. The overall translation is complete and accurate, reflecting a balance between creativity and precision. The presence of both biased tokens indicates the model was successfully guided to prioritize their usage.\n",
    "- Approach 2: The translation avoids the biased tokens “ejemplo” and “moderación” entirely, confirming the effectiveness of the negative logit_bias. The model instead uses alternative expressions like “donde” and “razones” to convey the same meaning. The shorter max_tokens resulted in a more concise translation without sacrificing much accuracy.\n",
    "\n",
    "#### Effect of logit_bias:\n",
    "- In Approach 1, the positive bias encouraged the use of the specified tokens, resulting in their inclusion.\n",
    "- In Approach 2, the negative bias effectively removed the tokens, forcing the model to find alternative phrasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Tool\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the text to analyze:  Instead of dwelling on negatives, actively seek out the good aspects of your day. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment: Positive\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text to analyze:  Instead of dwelling on negatives, actively seek out the good aspects of your day. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment(text, message_history,  temperature=0.5, max_tokens=50, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, logprobs=None):\n",
    "    # Create a prompt for Sentiment Analysis\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{text}\"})\n",
    "    \n",
    "    # Call OpenAI API\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=message_history,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n,\n",
    "        logprobs=logprobs\n",
    "    )\n",
    "\n",
    "    # Return the sentiment classification\n",
    "    sentiment = response.choices[0].message.content\n",
    "    return sentiment\n",
    "\n",
    "print(\"Sentiment Analysis Tool\\n\")\n",
    "input_text = input(\"Enter the text to analyze: \")\n",
    "\n",
    "# Initial messages: set up the assistant's role\n",
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Determine the sentiment of the following text (Positive, Negative, or Neutral)\"}\n",
    "        ]\n",
    "\n",
    "# Parameters for experimentation\n",
    "sentiment = analyze_sentiment(\n",
    "    text=input_text,\n",
    "    message_history=messages,\n",
    "    temperature=0.1,        # Minimal randomness for more factual sentiment detection\n",
    "    max_tokens=10,          # Keep responses short (just 'Positive', 'Negative', or 'Neutral')\n",
    "    top_p=1,                # Deterministic token selection\n",
    "    frequency_penalty=0,    # No penalty for repeating words\n",
    "    presence_penalty=0,     # No encouragement for new words\n",
    "    n=1,                    # Single response\n",
    "    logprobs=None           # No token-level probabilities needed\n",
    ")\n",
    "\n",
    "print(f\"\\nSentiment: {sentiment}\")\n",
    "\n",
    "# Initial messages: set up the assistant's role\n",
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Determine the sentiment of the following text (Positive, Negative, or Neutral)\"}\n",
    "        ]\n",
    "\n",
    "input_text = input(\"\\nEnter the text to analyze: \")\n",
    "\n",
    "# Parameters for experimentation\n",
    "sentiment = analyze_sentiment(\n",
    "    text=input_text,\n",
    "    message_history=messages,\n",
    "    temperature=0.6,        # Minimal randomness for more factual sentiment detection\n",
    "    max_tokens=10,          # Keep responses short (just 'Positive', 'Negative', or 'Neutral')\n",
    "    top_p=1,                # Deterministic token selection\n",
    "    frequency_penalty=0,    # No penalty for repeating words\n",
    "    presence_penalty=0,     # No encouragement for new words\n",
    "    n=1,                    # Single response\n",
    "    logprobs=None           # No token-level probabilities needed\n",
    ")\n",
    "\n",
    "print(f\"\\nSentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4\n",
    "\n",
    "- The output remains \"Positive\" across both parameter configurations due to the clear, optimistic wording of the input text.\n",
    "- Since the input text has a strong positive tone, both configurations with different temperatures correctly classified it as \"Positive.\"\n",
    "- If given a more ambiguous input (e.g., “Today was neither good nor bad”), the differences in temperature, top_p, and presence_penalty might have a stronger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Completion Tool\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the initial prompt:  Once upon a time in a small village\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text - Approach 1:\n",
      "Once upon a time in a small village nestled between rolling hills and shimmering streams, life was simple and peaceful. The villagers were a close-knit community, known for their kindness and their tradition of storytelling. Every evening, after the sun dipped below the horizon, families would gather around fires to share tales of heroes, mythical creatures, and lessons learned from the past.\n",
      "\n",
      "Generated Text - Approach 2:\n",
      "Once upon a time in a small village nestled between rolling hills and lush green forests, there lived a kind-hearted girl named Elara. The villagers adored her for her unwavering spirit and willingness to help others. Each morning, she would rise with the sun, tending to her family's garden, where vibrant flowers bloomed alongside fresh vegetables.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(text, message_history, temperature=0.5, max_tokens=100, top_p=1, frequency_penalty=0, presence_penalty=0, stop=None, best_of=1):\n",
    "    # Create a prompt for translation\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{text}\"})\n",
    "    \n",
    "    # Call OpenAI API for text completion\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=message_history,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        stop=stop,\n",
    "        # best_of=best_of\n",
    "    )\n",
    "    \n",
    "    # Return the generated text\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt_role = \"Generate a complete text, using the provided text as initial context.\"\n",
    "\n",
    "print(\"Text Completion Tool\\n\")\n",
    "\n",
    "# User provides the initial prompt for text generation\n",
    "input_prompt = input(\"Enter the initial prompt: \")\n",
    "\n",
    "# Initial messages: set up the assistant's role\n",
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt_role}\n",
    "        ]\n",
    "\n",
    "# Generate text with the first set of parameters\n",
    "completion_1 = generate_text(\n",
    "    text=input_prompt,\n",
    "    message_history=messages,\n",
    "    temperature=0.7,        # Balanced creativity\n",
    "    max_tokens=100,         # Limit the output length\n",
    "    top_p=1,                # No diversity restriction\n",
    "    frequency_penalty=0.2,  # Slightly discourage repetition\n",
    "    presence_penalty=0.3,   # Encourage new content generation\n",
    "    stop=[\"\\n\"],            # Stop when a new line is generated\n",
    "    best_of=1               # Return the best of 1 completion\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Text - Approach 1:\")\n",
    "print(completion_1)\n",
    "\n",
    "# Initial messages: set up the assistant's role\n",
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt_role}\n",
    "        ]\n",
    "\n",
    "# Generate text with a second set of parameters for experimentation\n",
    "completion_2 = generate_text(\n",
    "    text=input_prompt,\n",
    "    message_history=messages,\n",
    "    temperature=0.9,        # Higher creativity and randomness\n",
    "    max_tokens=100,          # Shorter output\n",
    "    top_p=0.85,             # Increase diversity in token selection\n",
    "    frequency_penalty=0,    # Allow repeating words if necessary\n",
    "    presence_penalty=0.6,   # Encourage introducing new ideas\n",
    "    stop=[\"\\n\"],             # Stop after completing a sentence\n",
    "    best_of=2               # Choose the best from 2 completions\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Text - Approach 2:\")\n",
    "print(completion_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
